{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9599119,"sourceType":"datasetVersion","datasetId":5855798}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The purpose of this notebook is to document how I clean a dataset in json format.\n\n**Issue to solve**: A key-value pair with multiple values cannot be read by Tableau when the json file was imported.","metadata":{}},{"cell_type":"markdown","source":"# NOTE: I wanted to try Pandas for data wrangling and exploratory data analysis, but it's not possible with the datasets being in a JSON format.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Read the report\ndf = pd.read_json(\"/kaggle/input/mapakalamidad-ph-api-datasets/archive.json\")\n\n# DataFrames' info\nprint(df.info());\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.002269Z","iopub.execute_input":"2024-10-16T12:34:53.003334Z","iopub.status.idle":"2024-10-16T12:34:53.172294Z","shell.execute_reply.started":"2024-10-16T12:34:53.003280Z","shell.execute_reply":"2024-10-16T12:34:53.170956Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 4 entries, type to bbox\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   statusCode  4 non-null      int64 \n 1   result      4 non-null      object\ndtypes: int64(1), object(1)\nmemory usage: 96.0+ bytes\nNone\n","output_type":"stream"},{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"         statusCode                                             result\ntype            200                                           Topology\nobjects         200  {'output': {'type': 'GeometryCollection', 'geo...\narcs            200                                                 []\nbbox            200  [118.7348612345, 6.1073926574, 126.0443688272,...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>statusCode</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>type</th>\n      <td>200</td>\n      <td>Topology</td>\n    </tr>\n    <tr>\n      <th>objects</th>\n      <td>200</td>\n      <td>{'output': {'type': 'GeometryCollection', 'geo...</td>\n    </tr>\n    <tr>\n      <th>arcs</th>\n      <td>200</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>bbox</th>\n      <td>200</td>\n      <td>[118.7348612345, 6.1073926574, 126.0443688272,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Notice that the table is horrible. That's why I opted to use the JSON library instead.","metadata":{}},{"cell_type":"code","source":"import json # data processing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T12:34:53.174685Z","iopub.execute_input":"2024-10-16T12:34:53.175659Z","iopub.status.idle":"2024-10-16T12:34:53.180597Z","shell.execute_reply.started":"2024-10-16T12:34:53.175596Z","shell.execute_reply":"2024-10-16T12:34:53.179366Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"# Importing the data from mapakalamidad.ph's API dataset","metadata":{}},{"cell_type":"markdown","source":"Run the cell below to read **current** reports:\n\n**Note:** One type of report at a time.","metadata":{}},{"cell_type":"code","source":"# CURRENT REPORTS\n# Read json file\nfile = open(\"/kaggle/input/mapakalamidad-ph-api-datasets/reports.json\", \"r\")\n\n# Read the data\ndata = file.read()\n\n# Parse the data\nreports = json.loads(data)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.182168Z","iopub.execute_input":"2024-10-16T12:34:53.182622Z","iopub.status.idle":"2024-10-16T12:34:53.196648Z","shell.execute_reply.started":"2024-10-16T12:34:53.182563Z","shell.execute_reply":"2024-10-16T12:34:53.195218Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"Run the cell below to read **archival** reports:\n\n**Note:** One type of report at a time.","metadata":{}},{"cell_type":"code","source":"# ARCHIVAL REPORTS (from January to February only)\n# Read json file\nfile = open(\"/kaggle/input/mapakalamidad-ph-api-datasets/archive.json\", \"r\")\n\n# Read the data\ndata = file.read()\n\n# Parse the data\nreports = json.loads(data)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.199512Z","iopub.execute_input":"2024-10-16T12:34:53.199979Z","iopub.status.idle":"2024-10-16T12:34:53.217561Z","shell.execute_reply.started":"2024-10-16T12:34:53.199922Z","shell.execute_reply":"2024-10-16T12:34:53.216420Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"markdown","source":"#  Step-by-tep process on how the wrangle function was coded","metadata":{}},{"cell_type":"markdown","source":"# Checking the reports\n\nIn json context, the data is an object. When it was loaded in python it is now read as a dictionary.\n\nNotice that I have to go through a hierarchy of keys (result -> objects -> output -> geometries) in order to access the reports.\n\nSince there are so many reports, I'm going to just check some of it:","metadata":{}},{"cell_type":"code","source":"count = 0  # counter\nfor report in reports[\"result\"][\"objects\"][\"output\"][\"geometries\"]:\n    count += 1\n    print(f\"{report}\\n\")\n    if count == 3: # loop will break after the third report\n        break","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.218667Z","iopub.execute_input":"2024-10-16T12:34:53.219043Z","iopub.status.idle":"2024-10-16T12:34:53.228582Z","shell.execute_reply.started":"2024-10-16T12:34:53.219002Z","shell.execute_reply":"2024-10-16T12:34:53.227297Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"{'type': 'Point', 'properties': {'pkey': '6876', 'created_at': '2023-02-21T14:32:36.888Z', 'source': 'grasp', 'status': 'confirmed', 'url': 'd44b36bc-df05-42e9-a39f-67937ccd5f9c', 'image_url': 'https://images.petabencana.id/d44b36bc-df05-42e9-a39f-67937ccd5f9c.jpg', 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 0}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-00'}, 'title': None, 'text': '#Youth4Bayanihan Jherome Domingo trained me'}, 'coordinates': [121.0099653179, 14.6980055343]}\n\n{'type': 'Point', 'properties': {'pkey': '6875', 'created_at': '2023-02-21T13:59:43.706Z', 'source': 'grasp', 'status': 'confirmed', 'url': '07b6935f-edf3-4dcd-b5fc-37d9cc491d57', 'image_url': None, 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 199}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-40'}, 'title': None, 'text': '#Youth4Bayanihan Maria Christina Labrador trained me'}, 'coordinates': [121.1313186627, 14.6854960897]}\n\n{'type': 'Point', 'properties': {'pkey': '6874', 'created_at': '2023-02-21T13:56:28.856Z', 'source': 'grasp', 'status': 'confirmed', 'url': 'cdf91cd1-5da0-4ce6-a46c-44d30c3cc28f', 'image_url': None, 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 200}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-00'}, 'title': None, 'text': '#Youth4Bayanihan Maria Christina Labrador trained me'}, 'coordinates': [120.994144324, 14.6691183928]}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"There are many keys in the report, but the `coordinates` key is what I should focus on. The problem is that Tableau won't read it because it contains a list.\n\nNote that Tableau only accepts single-valued attribute (since the coordinates contain a list, it will only read the first value).","metadata":{}},{"cell_type":"markdown","source":"# Cleaning the data\n\nBasically, I just have to separate the coordinates into latitude and longitude.\n\nNote: One of my friends cleaned the data manually (*he modified the contents of the file directly*.) I thought that it was tedious and I wanted to make it easier using the json library for future automation of cleaning the said reports.","metadata":{}},{"cell_type":"code","source":"## Sample of how it should be done\n\n# Get one sample of report\nsample_report = dict(reports[\"result\"][\"objects\"][\"output\"][\"geometries\"][0])\n\n# Get the latitude and longitude\nif \"coordinates\" in sample_report.keys():\n    lat, lon = sample_report[\"coordinates\"]\nprint(f\"{lat}, {lon}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.230223Z","iopub.execute_input":"2024-10-16T12:34:53.230673Z","iopub.status.idle":"2024-10-16T12:34:53.242535Z","shell.execute_reply.started":"2024-10-16T12:34:53.230626Z","shell.execute_reply":"2024-10-16T12:34:53.240788Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"121.0099653179, 14.6980055343\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that I have the latitude and longitude, I won't be needing the coordinates key.","metadata":{}},{"cell_type":"code","source":"# Removing the coordinates key\nprint(f\"Before removing the coordinate:\\n{sample_report}\\n\")\nif \"coordinates\" in sample_report.keys():\n    sample_report.pop(\"coordinates\")\nprint(f\"After removing the coordinate:\\n{sample_report}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.244355Z","iopub.execute_input":"2024-10-16T12:34:53.244837Z","iopub.status.idle":"2024-10-16T12:34:53.256150Z","shell.execute_reply.started":"2024-10-16T12:34:53.244782Z","shell.execute_reply":"2024-10-16T12:34:53.254888Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"Before removing the coordinate:\n{'type': 'Point', 'properties': {'pkey': '6876', 'created_at': '2023-02-21T14:32:36.888Z', 'source': 'grasp', 'status': 'confirmed', 'url': 'd44b36bc-df05-42e9-a39f-67937ccd5f9c', 'image_url': 'https://images.petabencana.id/d44b36bc-df05-42e9-a39f-67937ccd5f9c.jpg', 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 0}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-00'}, 'title': None, 'text': '#Youth4Bayanihan Jherome Domingo trained me'}, 'coordinates': [121.0099653179, 14.6980055343]}\n\nAfter removing the coordinate:\n{'type': 'Point', 'properties': {'pkey': '6876', 'created_at': '2023-02-21T14:32:36.888Z', 'source': 'grasp', 'status': 'confirmed', 'url': 'd44b36bc-df05-42e9-a39f-67937ccd5f9c', 'image_url': 'https://images.petabencana.id/d44b36bc-df05-42e9-a39f-67937ccd5f9c.jpg', 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 0}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-00'}, 'title': None, 'text': '#Youth4Bayanihan Jherome Domingo trained me'}}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Since the coordinate is gone, I will store the latitude and longitude so that I still have access to the geolocation of the report.","metadata":{}},{"cell_type":"code","source":"# Storing the lat and lon keys in the report\nsample_report[\"lat\"] = lat\nsample_report[\"lon\"] = lon\nsample_report","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.258248Z","iopub.execute_input":"2024-10-16T12:34:53.258687Z","iopub.status.idle":"2024-10-16T12:34:53.272999Z","shell.execute_reply.started":"2024-10-16T12:34:53.258643Z","shell.execute_reply":"2024-10-16T12:34:53.271397Z"},"trusted":true},"execution_count":144,"outputs":[{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"{'type': 'Point',\n 'properties': {'pkey': '6876',\n  'created_at': '2023-02-21T14:32:36.888Z',\n  'source': 'grasp',\n  'status': 'confirmed',\n  'url': 'd44b36bc-df05-42e9-a39f-67937ccd5f9c',\n  'image_url': 'https://images.petabencana.id/d44b36bc-df05-42e9-a39f-67937ccd5f9c.jpg',\n  'disaster_type': 'flood',\n  'report_data': {'report_type': 'flood', 'flood_depth': 0},\n  'tags': {'district_id': None,\n   'local_area_id': None,\n   'instance_region_code': 'PH-00'},\n  'title': None,\n  'text': '#Youth4Bayanihan Jherome Domingo trained me'},\n 'lat': 121.0099653179,\n 'lon': 14.6980055343}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Finally, the sample report is ready for analysis!","metadata":{}},{"cell_type":"markdown","source":"# Creating a wrangle function\n\nI have two api sources that I'm going to used:\n1. **. Current reports**\n2. **. Archival reports**\n\nSo, I'm going to create a function to make the process of cleaning the data from both sources wouldn't be repetitive.\n\nThe function will read and then clean the data. After the process is applied, it will save the reports in a separate file ready for visualization.","metadata":{}},{"cell_type":"code","source":"def wrangle(reports):\n    for report in reports[\"result\"][\"objects\"][\"output\"][\"geometries\"]:\n        # Get the latitude and longitude from the coordinate of the report\n        lat, lon = report[\"coordinates\"]\n        \n        # Drop the coordinates key from the report\n        report.pop(\"coordinates\")\n        \n        # Store the latitude and longitude keys in the report\n        report[\"lat\"] = lat\n        report[\"lon\"] = lon\n        \n    # Saving the reports in a new file\n    file_path = '/kaggle/working/cleaned.json'\n    with open(file_path, 'w', encoding='utf-8') as file:\n        json.dump(reports, file, ensure_ascii=False, indent=4)\n    \n    # Display the first three cleaned reports\n    count = 0  # counter\n    print(\"The first three reports from the dataset:\")\n    for report in reports[\"result\"][\"objects\"][\"output\"][\"geometries\"]:\n        count += 1\n        print(f\"{report}\\n\")\n        if count == 3: # loop will break after the third report\n            break\n    \n    return reports","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.274602Z","iopub.execute_input":"2024-10-16T12:34:53.275083Z","iopub.status.idle":"2024-10-16T12:34:53.286201Z","shell.execute_reply.started":"2024-10-16T12:34:53.275038Z","shell.execute_reply":"2024-10-16T12:34:53.285013Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"markdown","source":"I'll overwrite the reports with the archival ones. Let's try the wrangle function with it:","metadata":{}},{"cell_type":"code","source":"reports = wrangle(reports)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:34:53.289363Z","iopub.execute_input":"2024-10-16T12:34:53.289758Z","iopub.status.idle":"2024-10-16T12:34:53.355094Z","shell.execute_reply.started":"2024-10-16T12:34:53.289716Z","shell.execute_reply":"2024-10-16T12:34:53.353884Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"The first three reports from the dataset:\n{'type': 'Point', 'properties': {'pkey': '6876', 'created_at': '2023-02-21T14:32:36.888Z', 'source': 'grasp', 'status': 'confirmed', 'url': 'd44b36bc-df05-42e9-a39f-67937ccd5f9c', 'image_url': 'https://images.petabencana.id/d44b36bc-df05-42e9-a39f-67937ccd5f9c.jpg', 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 0}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-00'}, 'title': None, 'text': '#Youth4Bayanihan Jherome Domingo trained me'}, 'lat': 121.0099653179, 'lon': 14.6980055343}\n\n{'type': 'Point', 'properties': {'pkey': '6875', 'created_at': '2023-02-21T13:59:43.706Z', 'source': 'grasp', 'status': 'confirmed', 'url': '07b6935f-edf3-4dcd-b5fc-37d9cc491d57', 'image_url': None, 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 199}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-40'}, 'title': None, 'text': '#Youth4Bayanihan Maria Christina Labrador trained me'}, 'lat': 121.1313186627, 'lon': 14.6854960897}\n\n{'type': 'Point', 'properties': {'pkey': '6874', 'created_at': '2023-02-21T13:56:28.856Z', 'source': 'grasp', 'status': 'confirmed', 'url': 'cdf91cd1-5da0-4ce6-a46c-44d30c3cc28f', 'image_url': None, 'disaster_type': 'flood', 'report_data': {'report_type': 'flood', 'flood_depth': 200}, 'tags': {'district_id': None, 'local_area_id': None, 'instance_region_code': 'PH-00'}, 'title': None, 'text': '#Youth4Bayanihan Maria Christina Labrador trained me'}, 'lat': 120.994144324, 'lon': 14.6691183928}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It works! The cleaned reports are now in the /kaggle/working directory named \"cleaned.json\".","metadata":{}}]}